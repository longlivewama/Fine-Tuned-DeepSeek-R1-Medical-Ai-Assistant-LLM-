{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1500.86091,
      "end_time": "2024-09-25T13:34:07.208125",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-09-25T13:09:06.347215",
      "version": "2.6.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e256a423ceff4bdaa3dbc8a539abdc35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59f5b19bc2f84e61aba92f6a7cea37d5",
              "IPY_MODEL_48d70e103f2f438fb4214e70171713c5",
              "IPY_MODEL_60af60dcc9364225925c1cdf733aca0a"
            ],
            "layout": "IPY_MODEL_53e92ee13a6e4c1987f0fb29878654eb"
          }
        },
        "59f5b19bc2f84e61aba92f6a7cea37d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6633bb5329aa46daa0db4983eea1c1b1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5f31664abc15499782ef182bf7603cdf",
            "value": "Mapâ€‡(num_proc=2):â€‡100%"
          }
        },
        "48d70e103f2f438fb4214e70171713c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ae46dc52a554fc4811bac9ed72ef803",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac6862df823b4a8ea6e8d1d91b2a6480",
            "value": 500
          }
        },
        "60af60dcc9364225925c1cdf733aca0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8eb2b666e3ba4646bcf5711fc5fd252d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e5303a1f665b4c76b4c42e0a410b44d5",
            "value": "â€‡500/500â€‡[00:03&lt;00:00,â€‡183.34â€‡examples/s]"
          }
        },
        "53e92ee13a6e4c1987f0fb29878654eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6633bb5329aa46daa0db4983eea1c1b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f31664abc15499782ef182bf7603cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ae46dc52a554fc4811bac9ed72ef803": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac6862df823b4a8ea6e8d1d91b2a6480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8eb2b666e3ba4646bcf5711fc5fd252d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5303a1f665b4c76b4c42e0a410b44d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install relevant packages"
      ],
      "metadata": {
        "id": "GDoN535vvV8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import torch\n",
        "\n",
        "# Get GPU capability\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "\n",
        "# 1. Install Unsloth (no Kaggle extras needed in Colab)\n",
        "!pip install git+https://github.com/unslothai/unsloth.git\n",
        "\n",
        "# 2. Install stable versions of TRL and PEFT to avoid known bugs\n",
        "!pip install --no-deps \"trl<0.9.0\" \"peft<0.12.0\" \"accelerate>=0.31.0\" \"bitsandbytes>=0.43.1\"\n"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.status.busy": "2025-11-24T21:53:08.006239Z",
          "iopub.execute_input": "2025-11-24T21:53:08.006631Z",
          "iopub.status.idle": "2025-11-24T21:53:30.052096Z",
          "shell.execute_reply.started": "2025-11-24T21:53:08.006596Z",
          "shell.execute_reply": "2025-11-24T21:53:30.050746Z"
        },
        "papermill": {
          "duration": 164.985343,
          "end_time": "2024-09-25T13:11:54.926296",
          "exception": false,
          "start_time": "2024-09-25T13:09:09.940953",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "IaBelILIvV8r"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import all relevant packages throughout this walkthrough"
      ],
      "metadata": {
        "id": "GvINpgFrvV8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# -------------------------------\n",
        "# Modules for Fine-Tuning\n",
        "# -------------------------------\n",
        "def import_fine_tuning_modules():\n",
        "    global FastLanguageModel, torch, SFTTrainer, is_bfloat16_supported\n",
        "    from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "    import torch\n",
        "    from trl import SFTTrainer\n",
        "\n",
        "# -------------------------------\n",
        "# Hugging Face Modules\n",
        "# -------------------------------\n",
        "def import_huggingface_modules():\n",
        "    global login, TrainingArguments, load_dataset\n",
        "    from huggingface_hub import login\n",
        "    from transformers import TrainingArguments\n",
        "    from datasets import load_dataset\n",
        "\n",
        "# -------------------------------\n",
        "# Weights & Biases (WnB)\n",
        "# -------------------------------\n",
        "def import_wandb():\n",
        "    global wandb\n",
        "    import wandb\n",
        "\n",
        "# -------------------------------\n",
        "# Kaggle Secrets\n",
        "# -------------------------------\n",
        "def import_kaggle_secrets():\n",
        "    global UserSecretsClient\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "# -------------------------------\n",
        "# Call the functions to import all\n",
        "# -------------------------------\n",
        "import_fine_tuning_modules()\n",
        "import_huggingface_modules()\n",
        "import_wandb()\n",
        "import_kaggle_secrets()\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-24T21:53:30.053671Z",
          "iopub.execute_input": "2025-11-24T21:53:30.054303Z",
          "iopub.status.idle": "2025-11-24T21:53:42.830543Z",
          "shell.execute_reply.started": "2025-11-24T21:53:30.054276Z",
          "shell.execute_reply": "2025-11-24T21:53:42.829605Z"
        },
        "papermill": {
          "duration": 31.301473,
          "end_time": "2024-09-25T13:12:26.233487",
          "exception": false,
          "start_time": "2024-09-25T13:11:54.932014",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "5wgv0JLTvV8s",
        "outputId": "1af522f8-b250-48b8-b33c-6e80e6e73bff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# -------------------------------\\n# Modules for Fine-Tuning\\n# -------------------------------\\ndef import_fine_tuning_modules():\\n    global FastLanguageModel, torch, SFTTrainer, is_bfloat16_supported\\n    from unsloth import FastLanguageModel, is_bfloat16_supported\\n    import torch\\n    from trl import SFTTrainer\\n\\n# -------------------------------\\n# Hugging Face Modules\\n# -------------------------------\\ndef import_huggingface_modules():\\n    global login, TrainingArguments, load_dataset\\n    from huggingface_hub import login\\n    from transformers import TrainingArguments\\n    from datasets import load_dataset\\n\\n# -------------------------------\\n# Weights & Biases (WnB)\\n# -------------------------------\\ndef import_wandb():\\n    global wandb\\n    import wandb\\n\\n# -------------------------------\\n# Kaggle Secrets\\n# -------------------------------\\ndef import_kaggle_secrets():\\n    global UserSecretsClient\\n    from kaggle_secrets import UserSecretsClient\\n\\n# -------------------------------\\n# Call the functions to import all\\n# -------------------------------\\nimport_fine_tuning_modules()\\nimport_huggingface_modules()\\nimport_wandb()\\nimport_kaggle_secrets()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1) Load Secrets from uploaded local file (colab_secrets.json)\n",
        "# ============================================================\n",
        "\n",
        "import json\n",
        "\n",
        "with open(\"/content/colab_secrets.json\", \"r\") as f:\n",
        "    secrets = json.load(f)\n",
        "\n",
        "HF_TOKEN = secrets[\"HF_TOKEN\"]  # <-- your exact key name\n",
        "WANDB_API_KEY = secrets[\"Wnb\"]   # <-- your exact key name\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) Login to HuggingFace and Weights & Biases\n",
        "# ============================================================\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(HF_TOKEN)\n",
        "\n",
        "import wandb\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3) Import all modules needed for fine-tuning\n",
        "# ============================================================\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch  # PyTorch\n",
        "from trl import SFTTrainer  # Supervised fine-tuning trainer\n",
        "from unsloth import is_bfloat16_supported  # Check bf16 support\n",
        "\n",
        "from transformers import TrainingArguments  # Training hyperparameters\n",
        "from datasets import load_dataset  # Load fine-tuning datasets\n",
        "\n",
        "print(\"ðŸ”¥ All modules imported successfully and Secrets loaded!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8cLMpE1ya3R",
        "outputId": "4e403f52-1fb5-4e9f-847d-9967e9f24258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahmedfahim20004\u001b[0m (\u001b[33mahmedfahim20004-wama\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "ðŸ”¥ All modules imported successfully and Secrets loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create API keys and login to Hugging Face and Weights and Biases"
      ],
      "metadata": {
        "id": "P3x4y33YvV8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Initialize Hugging Face & WnB tokens\n",
        "# -------------------------------\n",
        "# user_secrets = UserSecretsClient()  # from kaggle_secrets import UserSecretsClient\n",
        "# hugging_face_token = user_secrets.get_secret(\"HF_Token\")\n",
        "# wnb_token = user_secrets.get_secret(\"wnb\")\n",
        "\n",
        "# -------------------------------\n",
        "# Login to Hugging Face\n",
        "# -------------------------------\n",
        "# login(hugging_face_token)  # from huggingface_hub import login\n",
        "\n",
        "# -------------------------------\n",
        "# Login to WnB\n",
        "# -------------------------------\n",
        "# wandb.login(key=wnb_token)  # import wandb\n",
        "# run = wandb.init(\n",
        "#     project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset_YouTube Walkthrough',\n",
        "#     job_type=\"training\",\n",
        "#     anonymous=\"allow\"\n",
        "# )\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-24T21:53:42.831897Z",
          "iopub.execute_input": "2025-11-24T21:53:42.832206Z",
          "iopub.status.idle": "2025-11-24T21:53:55.005354Z",
          "shell.execute_reply.started": "2025-11-24T21:53:42.832182Z",
          "shell.execute_reply": "2025-11-24T21:53:55.004633Z"
        },
        "papermill": {
          "duration": 0.618627,
          "end_time": "2024-09-25T13:12:26.85781",
          "exception": false,
          "start_time": "2024-09-25T13:12:26.239183",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "eH3J6F6ovV8t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= 1) Load Secrets from uploaded file =================\n",
        "import json\n",
        "\n",
        "# Load HuggingFace and WandB tokens from local file\n",
        "with open(\"/content/colab_secrets.json\", \"r\") as f:\n",
        "    secrets = json.load(f)\n",
        "\n",
        "hugging_face_token = secrets[\"HF_TOKEN\"]  # HuggingFace token\n",
        "wnb_token = secrets[\"Wnb\"]                # WandB API key\n",
        "\n",
        "\n",
        "# ================= 2) Login to Hugging Face & WandB =================\n",
        "from huggingface_hub import login\n",
        "login(hugging_face_token)  # Login to HuggingFace\n",
        "\n",
        "import wandb\n",
        "wandb.login(key=wnb_token)  # Login to Weights & Biases\n",
        "\n",
        "# Initialize WandB run\n",
        "run = wandb.init(\n",
        "    project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset_WAMA_code',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")\n",
        "\n",
        "\n",
        "# ================= 3) Import all modules needed for fine-tuning =================\n",
        "from unsloth import FastLanguageModel\n",
        "import torch  # PyTorch\n",
        "from trl import SFTTrainer  # Supervised fine-tuning trainer\n",
        "from unsloth import is_bfloat16_supported  # Check if hardware supports bf16\n",
        "\n",
        "from transformers import TrainingArguments  # Define training hyperparameters\n",
        "from datasets import load_dataset  # Load fine-tuning datasets\n",
        "\n",
        "print(\"ðŸ”¥ All modules imported successfully and Secrets loaded!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "QP2gxMDwzTuu",
        "outputId": "8bf7d92a-ebc8-4ee0-ef1f-c4df7dc8a965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251125_172949-plepjuaf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ahmedfahim20004-wama/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_WAMA_code/runs/plepjuaf?apiKey=f84f70f93607d8b3e4cd7478b982a1c90ff03c2a' target=\"_blank\">restful-shadow-3</a></strong> to <a href='https://wandb.ai/ahmedfahim20004-wama/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_WAMA_code?apiKey=f84f70f93607d8b3e4cd7478b982a1c90ff03c2a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ahmedfahim20004-wama/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_WAMA_code?apiKey=f84f70f93607d8b3e4cd7478b982a1c90ff03c2a' target=\"_blank\">https://wandb.ai/ahmedfahim20004-wama/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_WAMA_code?apiKey=f84f70f93607d8b3e4cd7478b982a1c90ff03c2a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ahmedfahim20004-wama/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_WAMA_code/runs/plepjuaf?apiKey=f84f70f93607d8b3e4cd7478b982a1c90ff03c2a' target=\"_blank\">https://wandb.ai/ahmedfahim20004-wama/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_WAMA_code/runs/plepjuaf?apiKey=f84f70f93607d8b3e4cd7478b982a1c90ff03c2a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Do NOT share these links with anyone. They can be used to claim your runs."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [openai] in use.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¥ All modules imported successfully and Secrets loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading DeepSeek R1 and the Tokenizer\n",
        "\n",
        "**What are we doing in this step?**\n",
        "\n",
        "In this step, we **load the DeepSeek R1 model and its tokenizer** using `FastLanguageModel.from_pretrained()`. We also **configure key parameters** for efficient inference and fine-tuning. We will be using a distilled 8B version of R1 for faster computation.  \n",
        "\n",
        "**Key parameters explained**\n",
        "```py\n",
        "max_seq_length = 2048  # Define the maximum sequence length a model can handle (i.e., number of tokens per input)\n",
        "dtype = None  # Default data type (usually auto-detected)\n",
        "load_in_4bit = True  # Enables 4-bit quantization â€“ a memory-saving optimization\n",
        "```\n",
        "\n",
        "**Intuition behind 4-bit quantization**\n",
        "\n",
        "Imagine compressing a **high-resolution image** to a smaller sizeâ€”**it takes up less space but still looks good enough**. Similarly, **4-bit quantization reduces the precision of model weights**, making the model **smaller and faster while keeping most of its accuracy**. Instead of storing precise **32-bit or 16-bit numbers**, we compress them into **4-bit values**. This allows **large language models to run efficiently on consumer GPUs** without needing massive amounts of memory."
      ],
      "metadata": {
        "id": "C7k5uoXlvV8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters\n",
        "max_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\n",
        "dtype = None # Set to default\n",
        "load_in_4bit = True # Enables 4 bit quantization â€” a memory saving optimization\n",
        "\n",
        "# Load the DeepSeek R1 model and tokenizer using unsloth â€” imported using: from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",  # Load the pre-trained DeepSeek R1 model (8B parameter version)\n",
        "    max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once\n",
        "    dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)\n",
        "    load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory\n",
        "    token=hugging_face_token, # Use hugging face token\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-24T21:53:55.00641Z",
          "iopub.execute_input": "2025-11-24T21:53:55.006731Z",
          "iopub.status.idle": "2025-11-24T21:54:05.079727Z",
          "shell.execute_reply.started": "2025-11-24T21:53:55.00669Z",
          "shell.execute_reply": "2025-11-24T21:54:05.079039Z"
        },
        "papermill": {
          "duration": 33.212874,
          "end_time": "2024-09-25T13:13:03.511347",
          "exception": false,
          "start_time": "2024-09-25T13:12:30.298473",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2hOth10vV8t",
        "outputId": "8d65b07a-3025-4435-8fd0-1bf562382373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing DeepSeek R1 on a medical use-case before fine-tuning\n",
        "\n",
        "\n",
        "### Defining a system prompt\n",
        "To create a prompt style for the model, we will define a system prompt and include placeholders for the question and response generation. The prompt will guide the model to think step-by-step and provide a logical, accurate response."
      ],
      "metadata": {
        "id": "Yo5FbutMvV8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a system prompt under prompt_style\n",
        "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>{}\"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:05.080497Z",
          "iopub.execute_input": "2025-11-24T21:54:05.080703Z",
          "iopub.status.idle": "2025-11-24T21:54:05.084873Z",
          "shell.execute_reply.started": "2025-11-24T21:54:05.080685Z",
          "shell.execute_reply": "2025-11-24T21:54:05.084126Z"
        },
        "papermill": {
          "duration": 0.014436,
          "end_time": "2024-09-25T13:13:09.483925",
          "exception": false,
          "start_time": "2024-09-25T13:13:09.469489",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "PJJ4FrkTvV8t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running inference on the model\n",
        "\n",
        "In this step, we **test the DeepSeek R1 model** by providing a **medical question** and generating a response.  \n",
        "The process involves the following steps:\n",
        "\n",
        "1. **Define a test question** related to a medical case.\n",
        "2. **Format the question using the structured prompt (`prompt_style`)** to ensure the model follows a logical reasoning process.\n",
        "3. **Tokenize the input and move it to the GPU (`cuda`)** for faster inference.\n",
        "4. **Generate a response using the model**, specifying key parameters like `max_new_tokens=1200` (limits response length).\n",
        "5. **Decode the output tokens back into text** to obtain the final readable answer."
      ],
      "metadata": {
        "id": "4D2hdFTDvV8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a test medical question for inference\n",
        "question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or\n",
        "              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\n",
        "              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n",
        "\n",
        "# Enable optimized inference mode for Unsloth models (improves speed and efficiency)\n",
        "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
        "\n",
        "# Format the question using the structured prompt (`prompt_style`) and tokenize it\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  # Convert input to PyTorch tensor & move to GPU\n",
        "\n",
        "# Generate a response using the model\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids, # Tokenized input question\n",
        "    attention_mask=inputs.attention_mask, # Attention mask to handle padding\n",
        "    max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n",
        "    use_cache=True, # Enable caching for faster inference\n",
        ")\n",
        "\n",
        "# Decode the generated output tokens into human-readable text\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "# Extract and print only the relevant response part (after \"### Response:\")\n",
        "print(response[0].split(\"### Response:\")[1])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:05.085875Z",
          "iopub.execute_input": "2025-11-24T21:54:05.086127Z",
          "iopub.status.idle": "2025-11-24T21:54:38.871921Z",
          "shell.execute_reply.started": "2025-11-24T21:54:05.086094Z",
          "shell.execute_reply": "2025-11-24T21:54:38.871133Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lq0X9Z_vV8u",
        "outputId": "52aa214a-c4a6-4408-9cf4-9901b36d46b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<think>\n",
            "Okay, so I'm trying to figure out what cystometry would show for this 61-year-old woman. Let me start by breaking down the information given.\n",
            "\n",
            "She has a history of involuntary urine loss, especially when she coughs or sneezes. That makes me think of stress urinary incontinence, which is common in women. But it's specifically during activities that put pressure on the bladder, so maybe it's due to urethral issues rather than bladder capacity. Also, she doesn't leak at night, which suggests that her bladder doesn't have problems holding urine when lying down, so it's less likely to be related to diurnal incontinence.\n",
            "\n",
            "She undergoes a gynecological exam and Q-tip test. I'm not super familiar with the Q-tip test, but I think it's used to check for urethral obstruction or uretal calculus. So maybe the Q-tip test was normal, indicating that there's no obstruction or stone causing the issue. Alternatively, if it was abnormal, that could point towards something else, like a narrowing of the urethra.\n",
            "\n",
            "Now, considering the possible causes of her symptoms. Since she leaks when coughing, it's more about the urethral sphincter not closing properly during these activities. So maybe she has urethral atrophy or some weakness in the sphincter muscles. Alternatively, there could be a prolapse of the bladder or urethra, but since she doesn't leak at night, that's less likely.\n",
            "\n",
            "Moving on to cystometry. Cystometry is a diagnostic test that measures how much urine the bladder can hold and how the muscles contract. It's usually done by filling the bladder with a small catheter and measuring the pressure as the bladder fills and empties. The key findings would be the residual volume and detrusor contractions.\n",
            "\n",
            "Residual volume refers to the amount of urine left in the bladder after emptying. If the residual volume is high, it might indicate that the bladder isn't emptying completely, which could contribute to incontinence. However, in this case, she doesn't leak at night, so maybe her residual volume is normal.\n",
            "\n",
            "Detrusor contractions are the contractions of the detrusor muscle that help push urine out of the bladder. If these contractions are weak or ineffective, it can lead to incomplete emptying and incontinence. If the detrusor is overactive, it might cause urge incontinence, but since her issues are more with stress incontinence, maybe the detrusor contractions are normal.\n",
            "\n",
            "Wait, but stress incontinence is more about urethral resistance. So maybe the issue is not with the bladder muscle but with the urethral sphincter. Therefore, the cystometry might show normal detrusor contractions but perhaps some issue with urethral function.\n",
            "\n",
            "Alternatively, if the Q-tip test was normal, indicating no obstruction, then the problem isn't in the urethral outlet. So maybe the issue is with the bladder's capacity or the sphincter's closure during activities.\n",
            "\n",
            "In that case, the residual volume might be normal because she doesn't leak at night, but the detrusor contractions might be normal too. However, the sphincter closure might be insufficient during activities like coughing, leading to leakage.\n",
            "\n",
            "So putting it all together, the cystometry would likely show normal residual volume and normal detrusor contractions, but the issue is elsewhere, possibly in sphincter function. Therefore, the answer would focus on the normalcy of those two aspects.\n",
            "</think>\n",
            "\n",
            "Based on the analysis, the cystometry is most likely to reveal normal residual volume and normal detrusor contractions for the 61-year-old woman. The findings suggest that the issue contributing to her involuntary urine loss during activities such as coughing or sneezing is not related to the bladder's capacity or detrusor muscle function but rather to the urethral sphincter's ability to close properly during these activities.<ï½œendâ–ofâ–sentenceï½œ>\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Before starting fine-tuning â€” why are we fine-tuning in the first place?**\n",
        ">\n",
        "> Even without fine-tuning, our model successfully generated a chain of thought and provided reasoning before delivering the final answer. The reasoning process is encapsulated within the `<think>` `</think>` tags. So, why do we still need fine-tuning? The reasoning process, while detailed, was long-winded and not concise. Additionally, we want the final answer to be consistent in a certain style.\n",
        "\n"
      ],
      "metadata": {
        "id": "17Dt4fj0vV8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning step by step\n",
        "\n",
        "### Step 1 â€” Update the system prompt\n",
        "We will slightly change the prompt style for processing the dataset by adding the third placeholder for the complex chain of thought column. `</think>`"
      ],
      "metadata": {
        "id": "9R2YDnrrvV8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated training prompt style to add </think> tag\n",
        "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}\"\"\"\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:38.872853Z",
          "iopub.execute_input": "2025-11-24T21:54:38.873075Z",
          "iopub.status.idle": "2025-11-24T21:54:38.87741Z",
          "shell.execute_reply.started": "2025-11-24T21:54:38.873053Z",
          "shell.execute_reply": "2025-11-24T21:54:38.876439Z"
        },
        "id": "sf2C5TYOvV8u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 â€” Download the fine-tuning dataset and format it for fine-tuning\n",
        "\n",
        "We will use the Medical O1 Reasoninng SFT found here on [Hugging Face](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT). From the authors: This dataset is used to fine-tune HuatuoGPT-o1, a medical LLM designed for advanced medical reasoning. This dataset is constructed using GPT-4o, which searches for solutions to verifiable medical problems and validates them through a medical verifier."
      ],
      "metadata": {
        "id": "DXnVfchWvV8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset using Hugging Face â€” function imported using from datasets import load_dataset\n",
        "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:500]\",trust_remote_code=True) # Keep only first 500 rows\n",
        "dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:38.880266Z",
          "iopub.execute_input": "2025-11-24T21:54:38.880516Z",
          "iopub.status.idle": "2025-11-24T21:54:39.48189Z",
          "shell.execute_reply.started": "2025-11-24T21:54:38.880493Z",
          "shell.execute_reply": "2025-11-24T21:54:39.481028Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEhLILF1vV8u",
        "outputId": "0a5ffcfe-3d45-4694-ab96-2d49db947655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'FreedomIntelligence/medical-o1-reasoning-SFT' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
            "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'FreedomIntelligence/medical-o1-reasoning-SFT' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['Question', 'Complex_CoT', 'Response'],\n",
              "    num_rows: 500\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Show an entry from the dataset\n",
        "dataset[1]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:39.483044Z",
          "iopub.execute_input": "2025-11-24T21:54:39.483301Z",
          "iopub.status.idle": "2025-11-24T21:54:39.489371Z",
          "shell.execute_reply.started": "2025-11-24T21:54:39.483279Z",
          "shell.execute_reply": "2025-11-24T21:54:39.488571Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9ACBlZxvV8v",
        "outputId": "b97a3f07-6330-4458-8211-9ede3ac62842"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Question': 'A 33-year-old woman is brought to the emergency department 15 minutes after being stabbed in the chest with a screwdriver. Given her vital signs of pulse 110/min, respirations 22/min, and blood pressure 90/65 mm Hg, along with the presence of a 5-cm deep stab wound at the upper border of the 8th rib in the left midaxillary line, which anatomical structure in her chest is most likely to be injured?',\n",
              " 'Complex_CoT': \"Okay, let's figure out what's going on here. A woman comes in with a stab wound from a screwdriver. It's in her chest, upper border of the 8th rib, left side, kind of around the midaxillary line. First thought, that's pretty close to where the lung sits, right?\\n\\nLet's talk about location first. This spot is along the left side of her body. Above the 8th rib, like that, is where a lot of important stuff lives, like the bottom part of the left lung, possibly the diaphragm too, especially considering how deep the screwdriver went.\\n\\nThe wound is 5 cm deep. That sounds pretty deep. I mean, it could definitely reach down to the lung tissue or maybe the diaphragm. Given that it's midaxillary, we're in the territory where the lower lobe of the left lung hangs out. It's also possible there's some intersection with where the diaphragm begins, but the lung feels more probable somehow.\\n\\nNow, her vitals are concerning: elevated heart rate and low blood pressure. This is serious. My gut says that this kind of vital sign picture could mean something like pneumothorax or maybe hemothorax. Both can happen if the lung gets punctured, and they can make the blood pressure drop and the heart rate skyrocket since she's obviously distressed.\\n\\nSo, putting it all together, the most obvious culprit is the lower lobe of the left lung. The wound's depth and her condition point that way. And, yeah, this adds up with pneumothorax or maybe blood collecting in the chestâ€”the kind of stuff that can really mess with breathing and circulation.\\n\\nAlright, with this in mind, it sure seems like the lung is the most likely thing that got hurt here. Makes sense given the situation, where the wound is, considering her symptoms, and the whole setup.\",\n",
              " 'Response': 'In this scenario, the most likely anatomical structure to be injured is the lower lobe of the left lung. The location of the stab woundâ€”at the upper border of the 8th rib in the left midaxillary lineâ€”indicates proximity to the lower lobe of the lung. The depth of the wound (5 cm) suggests it is sufficient to reach lung tissue. Her vital signs of elevated heart rate and low blood pressure could signal complications like a pneumothorax or hemothorax, common consequences of lung trauma that would result from a penetrating injury in this area. Given these considerations, the lower lobe of the left lung is the most probable structure injured.'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Next step is to structure the fine-tuning dataset according to train prompt styleâ€”why?**\n",
        ">\n",
        "> - Each question is paired with chain-of-thought reasoning and the final response.\n",
        "> - Ensures every training example follows a consistent pattern.\n",
        "> - Prevents the model from continuing beyond the expected response lengt by adding the EOS token."
      ],
      "metadata": {
        "id": "b61rf07uvV8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to format the dataset to fit our prompt training style\n",
        "EOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which the model when to stop generating text during training\n",
        "EOS_TOKEN"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:39.490333Z",
          "iopub.execute_input": "2025-11-24T21:54:39.490645Z",
          "iopub.status.idle": "2025-11-24T21:54:39.501745Z",
          "shell.execute_reply.started": "2025-11-24T21:54:39.490617Z",
          "shell.execute_reply": "2025-11-24T21:54:39.501087Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SEsd7pN2vV8v",
        "outputId": "79eb9ca6-2db4-49eb-c3ca-cf121c42bca1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<ï½œendâ–ofâ–sentenceï½œ>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define formatting prompt function\n",
        "def formatting_prompts_func(examples):  # Takes a batch of dataset examples as input\n",
        "    inputs = examples[\"Question\"]       # Extracts the medical question from the dataset\n",
        "    cots = examples[\"Complex_CoT\"]      # Extracts the chain-of-thought reasoning (logical step-by-step explanation)\n",
        "    outputs = examples[\"Response\"]      # Extracts the final model-generated response (answer)\n",
        "\n",
        "    texts = []  # Initializes an empty list to store the formatted prompts\n",
        "\n",
        "    # Iterate over the dataset, formatting each question, reasoning step, and response\n",
        "    for input, cot, output in zip(inputs, cots, outputs):\n",
        "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN  # Insert values into prompt template & append EOS token\n",
        "        texts.append(text)  # Add the formatted text to the list\n",
        "\n",
        "    return {\n",
        "        \"text\": texts,  # Return the newly formatted dataset with a \"text\" column containing structured prompts\n",
        "    }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:39.502543Z",
          "iopub.execute_input": "2025-11-24T21:54:39.502777Z",
          "iopub.status.idle": "2025-11-24T21:54:39.512024Z",
          "shell.execute_reply.started": "2025-11-24T21:54:39.502757Z",
          "shell.execute_reply": "2025-11-24T21:54:39.511214Z"
        },
        "id": "wdCyHB6xvV8v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Update dataset formatting\n",
        "dataset_finetune = dataset.map(formatting_prompts_func, batched = True)\n",
        "dataset_finetune[\"text\"][0]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:39.51295Z",
          "iopub.execute_input": "2025-11-24T21:54:39.513271Z",
          "iopub.status.idle": "2025-11-24T21:54:39.529052Z",
          "shell.execute_reply.started": "2025-11-24T21:54:39.513241Z",
          "shell.execute_reply": "2025-11-24T21:54:39.528328Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "hPxsh1RGvV8v",
        "outputId": "f8cc8d4c-1005-4405-d297-f19b1ebe1f8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nGiven the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\\n\\n### Response:\\n<think>\\nOkay, let's see what's going on here. We've got sudden weakness in the person's left arm and leg - and that screams something neuro-related, maybe a stroke?\\n\\nBut wait, there's more. The right lower leg is swollen and tender, which is like waving a big flag for deep vein thrombosis, especially after a long flight or sitting around a lot.\\n\\nSo, now I'm thinking, how could a clot in the leg end up causing issues like weakness or stroke symptoms?\\n\\nOh, right! There's this thing called a paradoxical embolism. It can happen if there's some kind of short circuit in the heart - like a hole that shouldn't be there.\\n\\nLet's put this together: if a blood clot from the leg somehow travels to the left side of the heart, it could shoot off to the brain and cause that sudden weakness by blocking blood flow there.\\n\\nHmm, but how would the clot get from the right side of the heart to the left without going through the lungs and getting filtered out?\\n\\nHere's where our cardiac anomaly comes in: a patent foramen ovale or PFO. That's like a sneaky little shortcut in the heart between the right and left atria.\\n\\nAnd it's actually pretty common, found in about a quarter of adults, which definitely makes it the top suspect here.\\n\\nSo with all these pieces - long travel, leg clot, sudden weakness - a PFO fits the bill perfectly, letting a clot cross over and cause all this.\\n\\nEverything fits together pretty neatly, so I'd bet PFO is the heart issue waiting to be discovered. Yeah, that really clicks into place!\\n</think>\\nThe specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.<ï½œendâ–ofâ–sentenceï½œ>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 â€” Setting up the model using LoRA\n",
        "\n",
        "**An intuitive explanation of LoRA**\n",
        "\n",
        "Large language models (LLMs) have **millions or even billions of weights** that determine how they process and generate text. When fine-tuning a model, we usually update all these weights, which **requires massive computational resources and memory**.\n",
        "\n",
        "LoRA (**Low-Rank Adaptation**) allows to fine-tune efficiently by:\n",
        "\n",
        "- Instead of modifying all weights, **LoRA adds small, trainable adapters** to specific layers.  \n",
        "- These adapters **capture task-specific knowledge** while leaving the original model unchanged.  \n",
        "- This reduces the number of trainable parameters **by more than 90%**, making fine-tuning **faster and more memory-efficient**.  \n",
        "\n",
        "Think of an LLM as a **complex factory**. Instead of rebuilding the entire factory to produce a new product, LoRA **adds small, specialized tools** to existing machines. This allows the factory to adapt quickly **without disrupting its core structure**.\n",
        "\n",
        "For a more technical explanation, check out this tutorial by [Sebastian Raschka](https://www.youtube.com/watch?v=rgmJep4Sb4&t).\n",
        "\n",
        "Below, we will use the `get_peft_model()` function which stands for Parameter-Efficient Fine-Tuning â€” this function wraps the base model (`model`) with LoRA modifications, ensuring that only specific parameters are trained."
      ],
      "metadata": {
        "id": "6kiOP0ZDvV8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA (Low-Rank Adaptation) fine-tuning to the model\n",
        "model_lora = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank: Determines the size of the trainable adapters (higher = more parameters, lower = more efficiency)\n",
        "    target_modules=[  # List of transformer layers where LoRA adapters will be applied\n",
        "        \"q_proj\",   # Query projection in the self-attention mechanism\n",
        "        \"k_proj\",   # Key projection in the self-attention mechanism\n",
        "        \"v_proj\",   # Value projection in the self-attention mechanism\n",
        "        \"o_proj\",   # Output projection from the attention layer\n",
        "        \"gate_proj\",  # Used in feed-forward layers (MLP)\n",
        "        \"up_proj\",    # Part of the transformerâ€™s feed-forward network (FFN)\n",
        "        \"down_proj\",  # Another part of the transformerâ€™s FFN\n",
        "    ],\n",
        "    lora_alpha=16,  # Scaling factor for LoRA updates (higher values allow more influence from LoRA layers)\n",
        "    lora_dropout=0,  # Dropout rate for LoRA layers (0 means no dropout, full retention of information)\n",
        "    bias=\"none\",  # Specifies whether LoRA layers should learn bias terms (setting to \"none\" saves memory)\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Saves memory by recomputing activations instead of storing them (recommended for long-context fine-tuning)\n",
        "    random_state=3407,  # Sets a seed for reproducibility, ensuring the same fine-tuning behavior across runs\n",
        "    use_rslora=False,  # Whether to use Rank-Stabilized LoRA (disabled here, meaning fixed-rank LoRA is used)\n",
        "    loftq_config=None,  # Low-bit Fine-Tuning Quantization (LoFTQ) is disabled in this configuration\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:39.529975Z",
          "iopub.execute_input": "2025-11-24T21:54:39.530255Z",
          "iopub.status.idle": "2025-11-24T21:54:45.709236Z",
          "shell.execute_reply.started": "2025-11-24T21:54:39.530223Z",
          "shell.execute_reply": "2025-11-24T21:54:45.708464Z"
        },
        "papermill": {
          "duration": 5.943269,
          "end_time": "2024-09-25T13:13:09.461941",
          "exception": false,
          "start_time": "2024-09-25T13:13:03.518672",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAHrzwLgvV8v",
        "outputId": "e008f4e1-1a32-4053-d0f8-b41d9b518c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.11.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we initialize `SFTTrainer`, a supervised fine-tuning trainer from `trl` (Transformer Reinforcement Learning), to fine-tune our model efficiently on a dataset."
      ],
      "metadata": {
        "id": "F1DDViN8vV8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize the fine-tuning trainer â€” Imported using from trl import SFTTrainer\n",
        "# trainer = SFTTrainer(\n",
        "#     model=model_lora,  # The model to be fine-tuned\n",
        "#     tokenizer=tokenizer,  # Tokenizer to process text inputs\n",
        "#     train_dataset=dataset_finetune,  # Dataset used for training\n",
        "#     dataset_text_field=\"text\",  # Specifies which field in the dataset contains training text\n",
        "#     max_seq_length=max_seq_length,  # Defines the maximum sequence length for inputs\n",
        "#     dataset_num_proc=2,  # Uses 2 CPU threads to speed up data preprocessing\n",
        "#\n",
        "#     # Define training arguments\n",
        "#     args=TrainingArguments(\n",
        "#         per_device_train_batch_size=2,  # Number of examples processed per device (GPU) at a time\n",
        "#         gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps before updating weights\n",
        "#         num_train_epochs=1, # Full fine-tuning run\n",
        "#         warmup_steps=5,  # Gradually increases learning rate for the first 5 steps\n",
        "#         max_steps=60,  # Limits training to 60 steps (useful for debugging; increase for full fine-tuning)\n",
        "#         learning_rate=2e-4,  # Learning rate for weight updates (tuned for LoRA fine-tuning)\n",
        "#         fp16=not is_bfloat16_supported(),  # Use FP16 (if BF16 is not supported) to speed up training\n",
        "#         bf16=is_bfloat16_supported(),  # Use BF16 if supported (better numerical stability on newer GPUs)\n",
        "#         logging_steps=10,  # Logs training progress every 10 steps\n",
        "#         optim=\"adamw_8bit\",  # Uses memory-efficient AdamW optimizer in 8-bit mode\n",
        "#         weight_decay=0.01,  # Regularization to prevent overfitting\n",
        "#         lr_scheduler_type=\"linear\",  # Uses a linear learning rate schedule\n",
        "#         seed=3407,  # Sets a fixed seed for reproducibility\n",
        "#         output_dir=\"outputs\",  # Directory where fine-tuned model checkpoints will be saved\n",
        "#     ),\n",
        "# )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:45.710003Z",
          "iopub.execute_input": "2025-11-24T21:54:45.710344Z",
          "iopub.status.idle": "2025-11-24T21:54:45.714699Z",
          "shell.execute_reply.started": "2025-11-24T21:54:45.710309Z",
          "shell.execute_reply": "2025-11-24T21:54:45.713922Z"
        },
        "id": "kex3FV-xvV8w"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Initialize the SFT Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model_lora,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset_finetune,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "\n",
        "        # Since we fixed the TRL version, we can safely use 4 again.\n",
        "        # (If you still face issues, revert this to 1).\n",
        "        gradient_accumulation_steps=4,\n",
        "\n",
        "        # Limit total steps for faster testing/debugging\n",
        "        max_steps=60,\n",
        "\n",
        "        warmup_steps=5,\n",
        "        learning_rate=2e-4,\n",
        "\n",
        "        # Automatic precision handling based on GPU support\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "\n",
        "        # Recommended to prevent memory leaks with WandB integration\n",
        "        report_to=\"wandb\",\n",
        "    ),\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:45.715591Z",
          "iopub.execute_input": "2025-11-24T21:54:45.71589Z",
          "iopub.status.idle": "2025-11-24T21:54:45.951571Z",
          "shell.execute_reply.started": "2025-11-24T21:54:45.715861Z",
          "shell.execute_reply": "2025-11-24T21:54:45.950882Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e256a423ceff4bdaa3dbc8a539abdc35",
            "59f5b19bc2f84e61aba92f6a7cea37d5",
            "48d70e103f2f438fb4214e70171713c5",
            "60af60dcc9364225925c1cdf733aca0a",
            "53e92ee13a6e4c1987f0fb29878654eb",
            "6633bb5329aa46daa0db4983eea1c1b1",
            "5f31664abc15499782ef182bf7603cdf",
            "4ae46dc52a554fc4811bac9ed72ef803",
            "ac6862df823b4a8ea6e8d1d91b2a6480",
            "8eb2b666e3ba4646bcf5711fc5fd252d",
            "e5303a1f665b4c76b4c42e0a410b44d5"
          ]
        },
        "id": "_uvDVMCFvV8w",
        "outputId": "867351ab-02a9-4797-8557-ad9b44270e80"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e256a423ceff4bdaa3dbc8a539abdc35"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 â€” Model training!\n",
        "\n",
        "This should take around 30 to 40 minutes â€” we can then check out our training results on Weights and Biases"
      ],
      "metadata": {
        "id": "ahg6fzjPvV8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the fine-tuning process\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:45.952396Z",
          "iopub.execute_input": "2025-11-24T21:54:45.95263Z",
          "iopub.status.idle": "2025-11-24T21:54:56.253988Z",
          "shell.execute_reply.started": "2025-11-24T21:54:45.952607Z",
          "shell.execute_reply": "2025-11-24T21:54:56.252721Z"
        },
        "papermill": {
          "duration": 1210.862521,
          "end_time": "2024-09-25T13:33:36.207919",
          "exception": false,
          "start_time": "2024-09-25T13:13:25.345398",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E3tFnBeAvV8w",
        "outputId": "df6b4d56-14e3-4800-d7a4-d19d203ac136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 500 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 19:43, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.148600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.151700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.206000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.044900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.291400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.179600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.141100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.217000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.994900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.007500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.105800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.970000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.870600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.056500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.927400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.898400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.817400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.897500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.807000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.847300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.936600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.913800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.812300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.767200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.814500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.688700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.874100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.746700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.779000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.816400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.785400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.687400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.748200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.849000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.744700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.723500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.589500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.688900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.759100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.700800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.797900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.788600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.774000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.683800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.716100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.686900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.664400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.606400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.819400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.620900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.721400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.599900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.665200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.673500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.742200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.623800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.651200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.698900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.640500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.659300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "wandb.finish()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:56.2545Z",
          "iopub.status.idle": "2025-11-24T21:54:56.254805Z",
          "shell.execute_reply": "2025-11-24T21:54:56.254688Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "epI4-FbSvV8w",
        "outputId": "f6b26e18-633a-4223-e294-b924ffd9476b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–â–‚â–‚â–‚â–ƒâ–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–…â–†â–…â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–â–‚â–‚â–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚</td></tr><tr><td>train/learning_rate</td><td>â–â–‚â–…â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–</td></tr><tr><td>train/loss</td><td>â–‡â–‡â–ˆâ–ˆâ–‡â–†â–†â–‡â–…â–„â–…â–…â–„â–„â–ƒâ–…â–ƒâ–„â–‚â–ƒâ–‚â–„â–ƒâ–ƒâ–â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–„â–‚â–â–‚â–â–‚â–‚â–‚</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.68242321903616e+16</td></tr><tr><td>train/epoch</td><td>0.96</td></tr><tr><td>train/global_step</td><td>60</td></tr><tr><td>train/grad_norm</td><td>1e-05</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6593</td></tr><tr><td>train_loss</td><td>1.83068</td></tr><tr><td>train_runtime</td><td>1217.2259</td></tr><tr><td>train_samples_per_second</td><td>0.394</td></tr><tr><td>train_steps_per_second</td><td>0.049</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">restful-shadow-3</strong> at: <a href='https://wandb.ai/ahmedfahim20004-wama/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_WAMA_code/runs/plepjuaf?apiKey=f84f70f93607d8b3e4cd7478b982a1c90ff03c2a' target=\"_blank\">https://wandb.ai/ahmedfahim20004-wama/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_WAMA_code/runs/plepjuaf?apiKey=f84f70f93607d8b3e4cd7478b982a1c90ff03c2a</a><br> View project at: <a href='https://wandb.ai/ahmedfahim20004-wama/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_WAMA_code?apiKey=f84f70f93607d8b3e4cd7478b982a1c90ff03c2a' target=\"_blank\">https://wandb.ai/ahmedfahim20004-wama/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_WAMA_code?apiKey=f84f70f93607d8b3e4cd7478b982a1c90ff03c2a</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251125_172949-plepjuaf/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 â€” Run model inference after fine-tuning"
      ],
      "metadata": {
        "id": "zLdBiLJavV8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing\n",
        "              but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\n",
        "              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n",
        "\n",
        "# Load the inference model using FastLanguageModel (Unsloth optimizes for speed)\n",
        "FastLanguageModel.for_inference(model_lora)  # Unsloth has 2x faster inference!\n",
        "\n",
        "# Tokenize the input question with a specific prompt format and move it to the GPU\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate a response using LoRA fine-tuned model with specific parameters\n",
        "outputs = model_lora.generate(\n",
        "    input_ids=inputs.input_ids,          # Tokenized input IDs\n",
        "    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n",
        "    max_new_tokens=1200,                  # Maximum length for generated response\n",
        "    use_cache=True,                        # Enable cache for efficient generation\n",
        ")\n",
        "\n",
        "# Decode the generated response from tokenized format to readable text\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "# Extract and print only the model's response part after \"### Response:\"\n",
        "print(response[0].split(\"### Response:\")[1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:56.255538Z",
          "iopub.status.idle": "2025-11-24T21:54:56.255812Z",
          "shell.execute_reply": "2025-11-24T21:54:56.255702Z"
        },
        "papermill": {
          "duration": 7.941358,
          "end_time": "2024-09-25T13:33:44.195877",
          "exception": false,
          "start_time": "2024-09-25T13:33:36.254519",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvUQRAgjvV8x",
        "outputId": "64701ede-6e21-4886-86c2-ae0360a0ca00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<think>\n",
            "Okay, so I'm trying to figure out what cystometry would show for this woman. She's 61 and has been dealing with involuntary urine loss whenever she coughs or sneezes, but she doesn't leak at night. She had a gynecological exam and a Q-tip test done. \n",
            "\n",
            "First, I know that involuntary urine loss during activities like coughing or sneezing is often related to an overactive bladder or hyperreflexia. That usually means the detrusor muscle is contracting more strongly and more frequently than it should. So, when they did the Q-tip test, they probably checked for bladder contractions.\n",
            "\n",
            "The Q-tip test involves inserting a catheter into the urethra, and then they measure how much the bladder contracts. If the contractions are too strong or too frequent, it might indicate hyperreflexia. \n",
            "\n",
            "Now, about the gynecological examâ€”this might have involved looking at the pelvic area to check for signs of prolapse or other structural issues. But since there's no mention of leakage at night, it's less likely that it's a retention issue.\n",
            "\n",
            "So, putting this together, I think cystometry would show that her detrusor contractions are too strong or happening too frequently. That would explain why she's having those involuntary losses during her activities. It's probably due to hyperreflexia, which means her bladder isn't behaving normally and is contracting involuntarily.\n",
            "\n",
            "Therefore, the most likely result from the cystometry would be that her residual volume is low because her detrusor contractions are strong and frequent, indicating hyperreflexia.\n",
            "</think>\n",
            "Cystometry would likely reveal that the woman's detrusor contractions are occurring involuntarily and frequently, indicating hyperreflexia. This condition explains her symptoms of involuntary urine loss during activities like coughing or sneezing, as her bladder is contracting too strongly and too frequently. The residual volume is likely low due to these strong contractions, and this aligns with her lack of leakage at night, which suggests that she doesn't experience significant retention issues. Therefore, the findings from the cystometry would confirm that her detrusor contractions are abnormal and responsible for her symptoms.<ï½œendâ–ofâ–sentenceï½œ>\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue,\n",
        "              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative,\n",
        "              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium.\n",
        "              What is the most likely predisposing factor for this patient's condition?\"\"\"\n",
        "\n",
        "# Tokenize the input question with a specific prompt format and move it to the GPU\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate a response using LoRA fine-tuned model with specific parameters\n",
        "outputs = model_lora.generate(\n",
        "    input_ids=inputs.input_ids,          # Tokenized input IDs\n",
        "    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n",
        "    max_new_tokens=1200,                  # Maximum length for generated response\n",
        "    use_cache=True,                        # Enable cache for efficient generation\n",
        ")\n",
        "\n",
        "# Decode the generated response from tokenized format to readable text\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "# Extract and print only the model's response part after \"### Response:\"\n",
        "print(response[0].split(\"### Response:\")[1])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-24T21:54:56.256449Z",
          "iopub.status.idle": "2025-11-24T21:54:56.256724Z",
          "shell.execute_reply": "2025-11-24T21:54:56.256615Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oBAU5I_vV8x",
        "outputId": "f3e99182-3b94-4dff-f4a3-f7b9b8d38bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<think>\n",
            "Alright, let's break this down step by step. \n",
            "\n",
            "First, the patient is a 59-year-old man presenting with symptoms like fever, chills, night sweats, and generalized fatigue. These symptoms are classic indicators of an active infection, likely a systemic one given the widespread nature of his fatigue.\n",
            "\n",
            "Next, the physical examination reveals a vegetation on the aortic valve measuring 12 mm. Vegetations on heart valves are most commonly associated with infections caused by bacteria, known as bacterial endocarditis. The presence of these vegetations suggests that the patient's condition is not due to a simple fever or infection elsewhere, but rather a more severe and systemic infection.\n",
            "\n",
            "Now, looking at the blood culture results: gram-positive, catalase-negative, gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium. Let's dissect this information.\n",
            "\n",
            "Gram-positive bacteria are a large group that includes many pathogenic bacteria. However, catalase-negative bacteria are notable because they lack the enzyme catalase, which typically breaks down hydrogen peroxide. This is a characteristic feature of the genus *Enterococcus*. The fact that these bacteria do not grow in a 6.5% NaCl medium is interesting. Staphylococcus aureus, for example, is known to grow in 6.5% NaCl, while *Enterococcus* species are more resistant to high salt concentrations. This suggests that the bacteria in question might not be *Staphylococcus* but rather *Enterococcus*.\n",
            "\n",
            "Putting this together, the patient has a bacterial infection that is likely caused by gram-positive, catalase-negative, gamma-hemolytic cocci in chains that do not grow in 6.5% NaCl. This profile strongly aligns with *Enterococcus faecium* or *Enterococcus faecalis*. These are known to be highly resistant to certain antibiotics and can cause serious infections, especially in patients with underlying conditions like valve disease.\n",
            "\n",
            "Considering that the patient has a vegetation on the aortic valve, this infection is most likely an example of bacterial endocarditis caused by *Enterococcus* species. This is particularly concerning because *Enterococcus* infections are associated with high rates of complications, including abscess formation, sepsis, and valve damage, and are often challenging to treat due to antibiotic resistance.\n",
            "\n",
            "In summary, the presence of gram-positive, catalase-negative, gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium, combined with the evidence of a vegetation on the aortic valve, strongly suggests that the patient's condition is due to bacterial endocarditis caused by *Enterococcus* species. This infection is a result of the body's immune response to the presence of these bacterial pathogens.\n",
            "</think>\n",
            "The patient's condition is most likely caused by bacterial endocarditis due to *Enterococcus* species. This is a result of the body's immune response to gram-positive, catalase-negative, gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium.<ï½œendâ–ofâ–sentenceï½œ>\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##save the model"
      ],
      "metadata": {
        "id": "L9pTb580rPvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Save the model and tokenizer locally (Save Locally)\n",
        "# Using saving_method=\"merged_16bit\" makes the model easier to use later but slightly larger in size\n",
        "# If you want a smaller size (LoRA-only), remove the merged_16bit line and keep save_pretrained normally\n",
        "print(\"ðŸ’¾ Saving model locally...\")\n",
        "model.save_pretrained(\"medical_chatbot_model\")\n",
        "tokenizer.save_pretrained(\"medical_chatbot_model\")\n",
        "\n",
        "# 2. Compress the model into a ZIP file\n",
        "print(\"ðŸ“¦ Zipping the model...\")\n",
        "!zip -r model_backup.zip medical_chatbot_model\n",
        "\n",
        "# 3. Download the file to your computer immediately\n",
        "print(\"â¬‡ï¸ Downloading to your computer...\")\n",
        "from google.colab import files\n",
        "files.download('model_backup.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "blu6VbLeDI71",
        "outputId": "fb17e7b4-8d99-4670-99c2-9a037edcf8db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¾ Saving model locally...\n",
            "ðŸ“¦ Zipping the model...\n",
            "  adding: medical_chatbot_model/ (stored 0%)\n",
            "  adding: medical_chatbot_model/chat_template.jinja (deflated 75%)\n",
            "  adding: medical_chatbot_model/model.safetensors.index.json (deflated 97%)\n",
            "  adding: medical_chatbot_model/tokenizer_config.json (deflated 96%)\n",
            "  adding: medical_chatbot_model/config.json (deflated 56%)\n",
            "  adding: medical_chatbot_model/generation_config.json (deflated 36%)\n",
            "  adding: medical_chatbot_model/tokenizer.json (deflated 85%)\n",
            "  adding: medical_chatbot_model/special_tokens_map.json (deflated 69%)\n",
            "  adding: medical_chatbot_model/model-00002-of-00002.safetensors (deflated 21%)\n",
            "  adding: medical_chatbot_model/model-00001-of-00002.safetensors (deflated 9%)\n",
            "â¬‡ï¸ Downloading to your computer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f100fb90-16c3-47dd-8a01-af3a1fc0e5f4\", \"model_backup.zip\", 5452485118)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('model_backup.zip')"
      ],
      "metadata": {
        "id": "ltvDgQkzlyuj",
        "outputId": "ade3ace3-5930-470d-a08f-e1aa3797d7a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3ff02d01-3f2a-49eb-aecf-e9bd080130af\", \"model_backup.zip\", 5452485118)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import TextIteratorStreamer\n",
        "from threading import Thread\n",
        "import os\n",
        "import random\n",
        "\n",
        "# 1. Enable inference mode\n",
        "try:\n",
        "    FastLanguageModel.for_inference(model_lora)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# --- Image setup ---\n",
        "MY_AVATAR_PATH = \"/content/doctor.jpg\"\n",
        "if os.path.exists(MY_AVATAR_PATH):\n",
        "    bot_avatar = MY_AVATAR_PATH\n",
        "else:\n",
        "    bot_avatar = \"https://cdn-icons-png.flaticon.com/512/1698/1698535.png\"\n",
        "\n",
        "# --- Core variables ---\n",
        "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "You are FEMOZ AI. Provide the diagnosis and treatment plan immediately. Be extremely concise.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}\"\"\"\n",
        "\n",
        "# --- Function to generate a short clinical case ---\n",
        "def generate_ai_case():\n",
        "    starters = [\"Male 50yo\", \"Female 30yo\", \"Child 6yo\", \"Elderly 75yo\"]\n",
        "    chosen_start = random.choice(starters)\n",
        "\n",
        "    creation_prompt = f\"\"\"### Instruction:\n",
        "Write a 2-sentence clinical vignette starting with \"{chosen_start}\".\n",
        "Include only: Chief complaint and Vitals. No fluff.\n",
        "\n",
        "### Response:\n",
        "<think>\n",
        "Generating short vignette...\n",
        "</think>\n",
        "\"\"\"\n",
        "    try:\n",
        "        inputs = tokenizer([creation_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "        outputs = model_lora.generate(\n",
        "            input_ids=inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=60,\n",
        "            temperature=0.8,\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "        text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        case = text.split(\"### Response:\")[-1].replace(\"<think>\", \"\").split(\"</think>\")[-1].strip()\n",
        "\n",
        "        if \".\" in case:\n",
        "            sentences = case.split(\".\")\n",
        "            case = sentences[0] + \".\" + (sentences[1] + \".\" if len(sentences) > 1 else \"\")\n",
        "\n",
        "        return case\n",
        "    except:\n",
        "        return \"Male 50yo, chest pain and sweating. BP 100/60.\"\n",
        "\n",
        "# --- Chat logic ---\n",
        "def medical_chat_stream(message, history):\n",
        "    if history is None: history = []\n",
        "    history.append([message, \"\"])\n",
        "\n",
        "    instruction = \"\"\"\n",
        "    (Format:\n",
        "     1. Diagnosis: [Name Only]\n",
        "     2. Plan: [3 Bullet Points Max]\n",
        "     Do not explain. Be direct.)\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        prompt = prompt_style.format(message + instruction, \"\", \"\")\n",
        "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "        generation_kwargs = dict(\n",
        "            input_ids=inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            streamer=streamer,\n",
        "            max_new_tokens=300,\n",
        "            do_sample=True,\n",
        "            temperature=0.4,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            no_repeat_ngram_size=3,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        thread = Thread(target=model_lora.generate, kwargs=generation_kwargs)\n",
        "        thread.start()\n",
        "\n",
        "        partial_text = \"\"\n",
        "        for new_text in streamer:\n",
        "            partial_text += new_text\n",
        "\n",
        "            # Hide the â€œthinkingâ€ block and show only the final result\n",
        "            formatted_text = partial_text.replace(\n",
        "                \"<think>\",\n",
        "                \"<span style='display:none'>\"\n",
        "            ).replace(\n",
        "                \"</think>\",\n",
        "                \"</span><div class='cyber-panel result'><div class='panel-header'>âš¡ RAPID DIAGNOSIS</div><div class='panel-content'>\"\n",
        "            )\n",
        "\n",
        "            if \"</think>\" in partial_text and not formatted_text.endswith(\"</div>\"):\n",
        "                 formatted_text += \"</div>\"\n",
        "\n",
        "            history[-1][1] = formatted_text\n",
        "            yield \"\", history\n",
        "\n",
        "    except Exception as e:\n",
        "        history[-1][1] = f\"âš ï¸ Error: {str(e)}\"\n",
        "        yield \"\", history\n",
        "\n",
        "# --- ðŸŽ¨ UI Design (Dark & Compact) ---\n",
        "custom_css = \"\"\"\n",
        ".gradio-container {\n",
        "    background-color: #0f172a !important;\n",
        "    font-family: 'Segoe UI', sans-serif !important;\n",
        "    max-width: 900px !important;\n",
        "    padding: 15px !important;\n",
        "    margin: 0 auto !important;\n",
        "}\n",
        "\n",
        ".header-cyber {\n",
        "    background: linear-gradient(to right, #1e293b, #0f172a);\n",
        "    border-bottom: 2px solid #3b82f6;\n",
        "    padding: 20px;\n",
        "    text-align: center;\n",
        "    margin-bottom: 15px;\n",
        "    border-radius: 8px;\n",
        "}\n",
        ".title-cyber {\n",
        "    color: #ffffff;\n",
        "    font-size: 2.2em;\n",
        "    font-weight: bold;\n",
        "    letter-spacing: 2px;\n",
        "    margin: 0;\n",
        "}\n",
        ".subtitle-cyber {\n",
        "    color: #94a3b8;\n",
        "    font-size: 0.9em;\n",
        "    margin-top: 5px;\n",
        "    letter-spacing: 1px;\n",
        "    text-transform: uppercase;\n",
        "}\n",
        "\n",
        ".chatbot {\n",
        "    height: 500px !important;\n",
        "    background-color: #1e293b !important;\n",
        "    border: 1px solid #334155 !important;\n",
        "    border-radius: 8px;\n",
        "}\n",
        "\n",
        "/* User message */\n",
        ".message-wrap .message.user {\n",
        "    background-color: #2563eb !important;\n",
        "    color: #ffffff !important;\n",
        "    border-radius: 6px;\n",
        "    padding: 10px 15px !important;\n",
        "    font-size: 14px;\n",
        "}\n",
        "\n",
        "/* Bot message container */\n",
        ".message-wrap .message.bot {\n",
        "    background-color: transparent !important;\n",
        "    border: none !important;\n",
        "    padding: 0 !important;\n",
        "}\n",
        "\n",
        "/* Diagnosis result panel */\n",
        ".cyber-panel.result {\n",
        "    border-left: 4px solid #10b981;\n",
        "    background: rgba(6, 78, 59, 0.3);\n",
        "    border-radius: 4px;\n",
        "    margin-top: 5px;\n",
        "}\n",
        ".cyber-panel.result .panel-header {\n",
        "    background: rgba(16, 185, 129, 0.1);\n",
        "    color: #34d399;\n",
        "    padding: 5px 10px;\n",
        "    font-weight: bold;\n",
        "    font-size: 0.85em;\n",
        "}\n",
        ".cyber-panel.result .panel-content {\n",
        "    padding: 10px;\n",
        "    color: #e2e8f0;\n",
        "    font-size: 1em;\n",
        "    line-height: 1.4;\n",
        "}\n",
        "\n",
        "/* Buttons */\n",
        "button.primary {\n",
        "    background-color: #3b82f6 !important;\n",
        "    color: white !important;\n",
        "    font-weight: bold;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "theme = gr.themes.Base(primary_hue=\"blue\", neutral_hue=\"slate\")\n",
        "\n",
        "with gr.Blocks(theme=theme, css=custom_css) as demo:\n",
        "\n",
        "    # New header with the required title\n",
        "    gr.HTML(\"\"\"\n",
        "    <div class=\"header-cyber\">\n",
        "        <div class=\"title-cyber\">FEMOZ AI</div>\n",
        "        <div class=\"subtitle-cyber\">CLINICAL DECISION SUPPORT SYSTEM</div>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=12):\n",
        "            chatbot = gr.Chatbot(\n",
        "                elem_classes=\"chatbot\",\n",
        "                show_label=False,\n",
        "                avatar_images=(None, bot_avatar),\n",
        "                render_markdown=True,\n",
        "                bubble_full_width=False\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                msg = gr.Textbox(\n",
        "                    placeholder=\"> Input data...\",\n",
        "                    show_label=False,\n",
        "                    container=False,\n",
        "                    scale=6,\n",
        "                    lines=1\n",
        "                )\n",
        "\n",
        "                with gr.Column(scale=3):\n",
        "                    with gr.Row():\n",
        "                        submit_btn = gr.Button(\"âš¡ SCAN\", variant=\"primary\", scale=2)\n",
        "                        # Button with the new name\n",
        "                        rand_btn = gr.Button(\"ðŸŽ² GENERATE CASE\", variant=\"secondary\", scale=1)\n",
        "                        clear_btn = gr.Button(\"âœ–ï¸\", variant=\"stop\", scale=1)\n",
        "\n",
        "    # Button connections\n",
        "    rand_btn.click(generate_ai_case, outputs=[msg])\n",
        "    msg.submit(medical_chat_stream, [msg, chatbot], [msg, chatbot])\n",
        "    submit_btn.click(medical_chat_stream, [msg, chatbot], [msg, chatbot])\n",
        "    clear_btn.click(lambda: ([], \"\"), outputs=[chatbot, msg])\n",
        "\n",
        "print(\"ðŸš€ Launching FEMOZ AI (Final CDSS Version)...\")\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "id": "yW-uNReV9Cnj",
        "outputId": "b13c05ab-2020-4e84-fe8d-f5f572af719b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4188371278.py:225: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-4188371278.py:225: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Launching FEMOZ AI (Final CDSS Version)...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://5fc01ffbbf41a28c78.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5fc01ffbbf41a28c78.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://5fc01ffbbf41a28c78.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AjiV7luj99Qu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}